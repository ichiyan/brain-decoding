{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5YoZNlQXxXGy",
        "2lQ2Jtq8Fz8g",
        "0neQc5WAF5zg",
        "_82c1a7YGask",
        "cdlioRVeGbCt",
        "e01wZRu4GUWr",
        "3DQiydKEGaSy"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "mount_file_id": "1RRLzG_U9-gsZggvkbbv_ran_z_jQzG4-",
      "authorship_tag": "ABX9TyNk8mfryKIk3lmZqwGJmAPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichiyan/brain-decoding/blob/master/vdvae_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation Set-Up"
      ],
      "metadata": {
        "id": "qbash1dvD90H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWKvkKtQ_zke",
        "outputId": "a770a9c5-ea1c-4155-eb7e-e43c01b98d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain-decoding'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Counting objects: 100% (334/334), done.\u001b[K\n",
            "remote: Compressing objects: 100% (257/257), done.\u001b[K\n",
            "remote: Total 334 (delta 69), reused 317 (delta 58), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (334/334), 13.18 MiB | 12.97 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ichiyan/brain-decoding.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHVj33bK-Gks",
        "outputId": "6723b3d2-fd1c-47bc-c755-2e08e6b0719d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zl0spf7FD4y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb29be6-3c9b-4a06-894f-921347712181"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MkOLZtH1DBC",
        "outputId": "4c33533d-f8ae-46b3-fda5-675dc6ad8379"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()"
      ],
      "metadata": {
        "id": "Z11y2QgI1MJl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)"
      ],
      "metadata": {
        "id": "JKOUcwFm1j8H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Nu2C-9aE9566",
        "outputId": "d547bb0c-3e48-4a2c-da25-518511dfe417"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/brain-decoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaPpDtxKxz7Q",
        "outputId": "b6303d49-d21d-49d1-c9d9-d37f72d184d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/latest/vdvae_extract_features.py -sub 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um6F4g0Nx2Yf",
        "outputId": "84a680b3-84fe-4c6e-8191-512e43279682"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libs imported\n",
            "Models Loading\n",
            "Restoring ema vae from vdvae/model/imagenet64-iter-1600000-model-ema.th\n",
            "0\n",
            "30\n",
            "60\n",
            "90\n",
            "120\n",
            "150\n",
            "180\n",
            "210\n",
            "240\n",
            "270\n",
            "300\n",
            "330\n",
            "360\n",
            "390\n",
            "420\n",
            "450\n",
            "480\n",
            "510\n",
            "540\n",
            "570\n",
            "600\n",
            "630\n",
            "660\n",
            "690\n",
            "720\n",
            "750\n",
            "780\n",
            "810\n",
            "840\n",
            "870\n",
            "900\n",
            "930\n",
            "960\n",
            "0\n",
            "30\n",
            "60\n",
            "90\n",
            "120\n",
            "150\n",
            "180\n",
            "210\n",
            "240\n",
            "270\n",
            "300\n",
            "330\n",
            "360\n",
            "390\n",
            "420\n",
            "450\n",
            "480\n",
            "510\n",
            "540\n",
            "570\n",
            "600\n",
            "630\n",
            "660\n",
            "690\n",
            "720\n",
            "750\n",
            "780\n",
            "810\n",
            "840\n",
            "870\n",
            "900\n",
            "930\n",
            "960\n",
            "990\n",
            "1020\n",
            "1050\n",
            "1080\n",
            "1110\n",
            "1140\n",
            "1170\n",
            "1200\n",
            "1230\n",
            "1260\n",
            "1290\n",
            "1320\n",
            "1350\n",
            "1380\n",
            "1410\n",
            "1440\n",
            "1470\n",
            "1500\n",
            "1530\n",
            "1560\n",
            "1590\n",
            "1620\n",
            "1650\n",
            "1680\n",
            "1710\n",
            "1740\n",
            "1770\n",
            "1800\n",
            "1830\n",
            "1860\n",
            "1890\n",
            "1920\n",
            "1950\n",
            "1980\n",
            "2010\n",
            "2040\n",
            "2070\n",
            "2100\n",
            "2130\n",
            "2160\n",
            "2190\n",
            "2220\n",
            "2250\n",
            "2280\n",
            "2310\n",
            "2340\n",
            "2370\n",
            "2400\n",
            "2430\n",
            "2460\n",
            "2490\n",
            "2520\n",
            "2550\n",
            "2580\n",
            "2610\n",
            "2640\n",
            "2670\n",
            "2700\n",
            "2730\n",
            "2760\n",
            "2790\n",
            "2820\n",
            "2850\n",
            "2880\n",
            "2910\n",
            "2940\n",
            "2970\n",
            "3000\n",
            "3030\n",
            "3060\n",
            "3090\n",
            "3120\n",
            "3150\n",
            "3180\n",
            "3210\n",
            "3240\n",
            "3270\n",
            "3300\n",
            "3330\n",
            "3360\n",
            "3390\n",
            "3420\n",
            "3450\n",
            "3480\n",
            "3510\n",
            "3540\n",
            "3570\n",
            "3600\n",
            "3630\n",
            "3660\n",
            "3690\n",
            "3720\n",
            "3750\n",
            "3780\n",
            "3810\n",
            "3840\n",
            "3870\n",
            "3900\n",
            "3930\n",
            "3960\n",
            "3990\n",
            "4020\n",
            "4050\n",
            "4080\n",
            "4110\n",
            "4140\n",
            "4170\n",
            "4200\n",
            "4230\n",
            "4260\n",
            "4290\n",
            "4320\n",
            "4350\n",
            "4380\n",
            "4410\n",
            "4440\n",
            "4470\n",
            "4500\n",
            "4530\n",
            "4560\n",
            "4590\n",
            "4620\n",
            "4650\n",
            "4680\n",
            "4710\n",
            "4740\n",
            "4770\n",
            "4800\n",
            "4830\n",
            "4860\n",
            "4890\n",
            "4920\n",
            "4950\n",
            "4980\n",
            "5010\n",
            "5040\n",
            "5070\n",
            "5100\n",
            "5130\n",
            "5160\n",
            "5190\n",
            "5220\n",
            "5250\n",
            "5280\n",
            "5310\n",
            "5340\n",
            "5370\n",
            "5400\n",
            "5430\n",
            "5460\n",
            "5490\n",
            "5520\n",
            "5550\n",
            "5580\n",
            "5610\n",
            "5640\n",
            "5670\n",
            "5700\n",
            "5730\n",
            "5760\n",
            "5790\n",
            "5820\n",
            "5850\n",
            "5880\n",
            "5910\n",
            "5940\n",
            "5970\n",
            "6000\n",
            "6030\n",
            "6060\n",
            "6090\n",
            "6120\n",
            "6150\n",
            "6180\n",
            "6210\n",
            "6240\n",
            "6270\n",
            "6300\n",
            "6330\n",
            "6360\n",
            "6390\n",
            "6420\n",
            "6450\n",
            "6480\n",
            "6510\n",
            "6540\n",
            "6570\n",
            "6600\n",
            "6630\n",
            "6660\n",
            "6690\n",
            "6720\n",
            "6750\n",
            "6780\n",
            "6810\n",
            "6840\n",
            "6870\n",
            "6900\n",
            "6930\n",
            "6960\n",
            "6990\n",
            "7020\n",
            "7050\n",
            "7080\n",
            "7110\n",
            "7140\n",
            "7170\n",
            "7200\n",
            "7230\n",
            "7260\n",
            "7290\n",
            "7320\n",
            "7350\n",
            "7380\n",
            "7410\n",
            "7440\n",
            "7470\n",
            "7500\n",
            "7530\n",
            "7560\n",
            "7590\n",
            "7620\n",
            "7650\n",
            "7680\n",
            "7710\n",
            "7740\n",
            "7770\n",
            "7800\n",
            "7830\n",
            "7860\n",
            "7890\n",
            "7920\n",
            "7950\n",
            "7980\n",
            "8010\n",
            "8040\n",
            "8070\n",
            "8100\n",
            "8130\n",
            "8160\n",
            "8190\n",
            "8220\n",
            "8250\n",
            "8280\n",
            "8310\n",
            "8340\n",
            "8370\n",
            "8400\n",
            "8430\n",
            "8460\n",
            "8490\n",
            "8520\n",
            "8550\n",
            "8580\n",
            "8610\n",
            "8640\n",
            "8670\n",
            "8700\n",
            "8730\n",
            "8760\n",
            "8790\n",
            "8820\n",
            "8850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/latest/vdvae_extract_depth_features_v2.py -sub 1"
      ],
      "metadata": {
        "id": "lUE5qIcD0Dp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/latest/vdvae_regression.py -sub 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04LJhCL71u0-",
        "outputId": "08f67319-01fc-422d-9670-ce3018791367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-4.8202814e-08 0.9999432\n",
            "-0.027261836 0.99147815\n",
            "24.480553 -14.412728\n",
            "9.81828 -9.315342\n",
            "Training latents Feature Regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/latest/vdvae_regression_depth_v2.py -sub 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzTDyV_d1vgn",
        "outputId": "5746dc16-2363-4d97-daa8-23d6a584f05a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-4.8202814e-08 0.9999432\n",
            "-0.027261836 0.99147815\n",
            "24.480553 -14.412728\n",
            "9.81828 -9.315342\n",
            "Training latents Feature Regression\n",
            "-0.01379516962094846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/latest/vdvae_reconstruct_images.py -sub 1"
      ],
      "metadata": {
        "id": "LwJG61_y1wOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/latest/vdvae_reconstruct_depth_images_v2.py -sub 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GQjI8_d1xAC",
        "outputId": "05dfa0ee-8a89-4701-8bcf-8dc5a08cb6ce"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libs imported\n",
            "Models Loading\n",
            "Restoring ema vae from vdvae/model/imagenet64-iter-1600000-model-ema.th\n",
            "0\n",
            "30\n",
            "60\n",
            "90\n",
            "120\n",
            "150\n",
            "180\n",
            "210\n",
            "240\n",
            "270\n",
            "300\n",
            "330\n",
            "360\n",
            "390\n",
            "420\n",
            "450\n",
            "480\n",
            "510\n",
            "540\n",
            "570\n",
            "600\n",
            "630\n",
            "660\n",
            "690\n",
            "720\n",
            "750\n",
            "780\n",
            "810\n",
            "840\n",
            "870\n",
            "900\n",
            "930\n",
            "960\n",
            "0\n",
            "30\n",
            "60\n",
            "90\n",
            "120\n",
            "150\n",
            "180\n",
            "210\n",
            "240\n",
            "270\n",
            "300\n",
            "330\n",
            "360\n",
            "390\n",
            "420\n",
            "450\n",
            "480\n",
            "510\n",
            "540\n",
            "570\n",
            "600\n",
            "630\n",
            "660\n",
            "690\n",
            "720\n",
            "750\n",
            "780\n",
            "810\n",
            "840\n",
            "870\n",
            "900\n",
            "930\n",
            "960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COPY FILES TO DRIVE"
      ],
      "metadata": {
        "id": "kEZp0Dp4Tk1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images\n",
        "\n"
      ],
      "metadata": {
        "id": "E1IvgiMScqvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cp /content/brain-decoding/results/vdvae/subj01/stim_43l/115.png /content/drive/MyDrive/brain_decoding/data/reconstructed/43l\n",
        "cp /content/brain-decoding/results/vdvae/subj01/stim_43l/25.png /content/drive/MyDrive/brain_decoding/data/reconstructed/43l\n",
        "cp /content/brain-decoding/results/vdvae/subj01/stim_43l/451.png /content/drive/MyDrive/brain_decoding/data/reconstructed/43l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8GFGR6NNpgN",
        "outputId": "04e295c9-d4ad-4813-ef1d-6c965bd56769"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/results/vdvae/subj01/stim_41l /content/drive/MyDrive/brain_decoding/data/reconstructed/41l"
      ],
      "metadata": {
        "id": "JmANR2J-Uens"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth"
      ],
      "metadata": {
        "id": "2mTwffIOcwFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cp /content/brain-decoding/results/vdvae/subj01/depth_stim_43l/115.png /content/drive/MyDrive/brain_decoding/data/reconstructed/41l-depth\n",
        "cp /content/brain-decoding/results/vdvae/subj01/depth_stim_43l/25.png /content/drive/MyDrive/brain_decoding/data/reconstructed/41l-depth\n",
        "cp /content/brain-decoding/results/vdvae/subj01/depth_stim_43l/451.png /content/drive/MyDrive/brain_decoding/data/reconstructed/41l-depth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt9OrRBIcZh7",
        "outputId": "e391473d-566a-48aa-81ce-17dea7490e93"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/results/vdvae/subj01/depth_stim_43l /content/drive/MyDrive/brain_decoding/data/reconstructed/41l-depth"
      ],
      "metadata": {
        "id": "rTlMc3VocMtC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracted Features"
      ],
      "metadata": {
        "id": "ddrl6zVVahyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/data/extracted_features/subj01/nsd_vdvae_depth_features_43l.npz /content/drive/MyDrive/brain_decoding/data"
      ],
      "metadata": {
        "id": "-YeCtbHRZCOe"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicted Features"
      ],
      "metadata": {
        "id": "Lq6oZPThamKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/data/predicted_features/subj01/nsd_vdvae_depth_nsdgeneral_pred_sub1_43l_alpha100k.npy /content/drive/MyDrive/brain_decoding/data"
      ],
      "metadata": {
        "id": "WLLVfdTuafnF"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "5YoZNlQXxXGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-colab-shell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW6ji4YQeQ1n",
        "outputId": "ee0741c3-9391-400e-8a3c-befcbe5d0488"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab-shell in /usr/local/lib/python3.10/dist-packages (0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbRvVFW1A4b9",
        "outputId": "68d2e3fa-8829-4e25-a1cc-8d2404a26e54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m2.3/2.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746500 sha256=7fec02dbb2ef73121e768ea2211ecfbd71e7191b18cade6ee7536dfb804b0521\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing python files from github"
      ],
      "metadata": {
        "id": "5XV6iodCElZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,'/content/brain-decoding/vdvae')"
      ],
      "metadata": {
        "id": "B5qyP7o3Eg-Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hps import Hyperparams, parse_args_and_update_hparams, add_vae_arguments"
      ],
      "metadata": {
        "id": "2MhVZEcVFIsc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from google_colab_shell import getshell\n",
        "# os.system('/content/brain-decoding/vdvae/setup_cifar10.sh')\n",
        "# os.system('/content/brain-decoding/vdvae/setup_ffhq1024.sh')\n",
        "# os.system('/content/brain-decoding/vdvae/setup_ffhq256.sh')\n",
        "# os.system('/content/brain-decoding/vdvae/setup_imagenet.sh')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlt5cPLvaama",
        "outputId": "e2637b97-7a23-43d9-8cf4-0c27403e4688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32256"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "bash /content/brain-decoding/vdvae/setup_cifar10.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfwy2lHBfH2a",
        "outputId": "c205f9ba-3346-478d-98d2-5b7a2abc1f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 14:14:35--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  32.1MB/s    in 5.7s    \n",
            "\n",
            "2023-10-27 14:14:41 (28.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/cifar-10-python.tar.gz /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "ThOfA6rdzY_s"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%shell\n",
        "# bash /content/brain-decoding/vdvae/setup_imagenet.sh -imagenet64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXuPFvGVfdTp",
        "outputId": "4d9162d3-e07f-47a7-b2d1-6d761bb78ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "please pass the string imagenet32 or imagenet64 as an argument\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir train\n",
        "# cp /content/brain-decoding/vdvae/train.py /content/train.py"
      ],
      "metadata": {
        "id": "vcdNjFczh4aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/brain-decoding/vdvae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plpjiOuHjqCi",
        "outputId": "14b12996-e084-4e35-eb86-8634598b01c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding/vdvae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>ImageNet 64</h3>"
      ],
      "metadata": {
        "id": "waxSILPuBbbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# 125M parameter model, trained for 1.6M iters (about 2.5 weeks on 32 V100)\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-log.jsonl\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model-ema.th\n",
        "# should be 2.44 nats, or 3.52 bits per dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S66MhH9cghba",
        "outputId": "94eb5cb2-bfb3-47e2-908c-71e4cef76e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 15:00:44--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-log.jsonl\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145115 (142K) [application/octet-stream]\n",
            "Saving to: ‘imagenet64-iter-1600000-log.jsonl’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 141.71K   286KB/s    in 0.5s    \n",
            "\n",
            "2023-10-27 15:00:45 (286 KB/s) - ‘imagenet64-iter-1600000-log.jsonl’ saved [145115/145115]\n",
            "\n",
            "--2023-10-27 15:00:45--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model.th\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 501006513 (478M) [text/plain]\n",
            "Saving to: ‘imagenet64-iter-1600000-model.th’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 477.80M  15.7MB/s    in 35s     \n",
            "\n",
            "2023-10-27 15:01:21 (13.7 MB/s) - ‘imagenet64-iter-1600000-model.th’ saved [501006513/501006513]\n",
            "\n",
            "--2023-10-27 15:01:21--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-model-ema.th\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 500977841 (478M) [text/plain]\n",
            "Saving to: ‘imagenet64-iter-1600000-model-ema.th’\n",
            "\n",
            "imagenet64-iter-160 100%[===================>] 477.77M  8.06MB/s    in 51s     \n",
            "\n",
            "2023-10-27 15:02:13 (9.40 MB/s) - ‘imagenet64-iter-1600000-model-ema.th’ saved [500977841/500977841]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/imagenet64-iter-1600000-opt.th\n",
        "python train.py --hps imagenet64 --restore_path imagenet64-iter-1600000-model.th --restore_ema_path imagenet64-iter-1600000-model-ema.th --restore_log_path imagenet64-iter-1600000-log.jsonl --restore_optimizer_path imagenet64-iter-1600000-opt.th --test_eval\n",
        "# should be 2.44 nats, or 3.52 bits per dim"
      ],
      "metadata": {
        "id": "lBgq4L07l5Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copying files from drive to repo dir"
      ],
      "metadata": {
        "id": "yWTF75tw20PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-log.jsonl /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-log.jsonl"
      ],
      "metadata": {
        "id": "WMTM64tG11_a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-model-ema.th /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-model-ema.th"
      ],
      "metadata": {
        "id": "0pjlZ9Ae1-en"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-model.th /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-model.th"
      ],
      "metadata": {
        "id": "MkiY7mGG2Ci0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-opt.th /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-opt.thz"
      ],
      "metadata": {
        "id": "CRkPkoc71Ahd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>FFHQ-256</h3>"
      ],
      "metadata": {
        "id": "ns8sm8nX3Iwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/brain-decoding/vdvae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOaO1EAU6ovo",
        "outputId": "afc99d27-6fa5-4fa2-9314-262ce0df7a4b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding/vdvae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "bash /content/brain-decoding/vdvae/setup_ffhq256.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nOhZ5q-EB73",
        "outputId": "9f2967c3-977b-44ff-f200-a1ccae551e92"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 16:56:28--  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq-256.npy\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13762560128 (13G) [text/plain]\n",
            "Saving to: ‘ffhq-256.npy’\n",
            "\n",
            "ffhq-256.npy         87%[================>   ]  11.24G  --.-KB/s    in 29m 1s  \n",
            "\n",
            "2023-10-27 17:25:29 (6.61 MB/s) - Read error at byte 12065636352/13762560128 (Connection timed out). Retrying.\n",
            "\n",
            "--2023-10-27 17:25:30--  (try: 2)  https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq-256.npy\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13762560128 (13G) [text/plain]\n",
            "Saving to: ‘ffhq-256.npy’\n",
            "\n",
            "ffhq-256.npy        100%[===================>]  12.82G  15.3MB/s    in 15m 37s \n",
            "\n",
            "2023-10-27 17:41:08 (14.0 MB/s) - ‘ffhq-256.npy’ saved [13762560128/13762560128]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# 115M parameters, trained for 1.7M iterations (or about 2.5 weeks) on 32 V100\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-log.jsonl\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-model.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-model-ema.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq256-iter-1700000-opt.th\n",
        "python train.py --hps ffhq256 --restore_path ffhq256-iter-1700000-model.th --restore_ema_path ffhq256-iter-1700000-model-ema.th --restore_log_path ffhq256-iter-1700000-log.jsonl --restore_optimizer_path ffhq256-iter-1700000-opt.th --test_eval\n",
        "# should be 0.4232 nats, or 0.61 bits per dim"
      ],
      "metadata": {
        "id": "2XAgBAT5wpxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq256-iter-1700000-log.jsonl /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "Bt66H2El8zNZ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq256-iter-1700000-model-ema.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "oV57LZ-487t1"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq256-iter-1700000-model.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "LJ7G_Qnr88Ll"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq256-iter-1700000-opt.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "tcWDow0R88jU"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>FFHQ-1024</h3>"
      ],
      "metadata": {
        "id": "YHqx6b_53ecN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "bash /content/brain-decoding/vdvae/setup_ffhq1024.sh"
      ],
      "metadata": {
        "id": "NFb4PJH4JP4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# 115M parameters, trained for 1.7M iterations (or about 2.5 weeks) on 32 V100\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-log.jsonl\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-model.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-model-ema.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets/ffhq1024-iter-1700000-opt.th\n",
        "python train.py --hps ffhq1024 --restore_path ffhq1024-iter-1700000-model.th --restore_ema_path ffhq1024-iter-1700000-model-ema.th --restore_log_path ffhq1024-iter-1700000-log.jsonl --restore_optimizer_path ffhq1024-iter-1700000-opt.th --test_eval\n",
        "# should be 1.678 nats, or 2.42 bits per dim"
      ],
      "metadata": {
        "id": "s2VSnjYi3h_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq1024-iter-1700000-log.jsonl /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "L2aCdkd8QtaP"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq1024-iter-1700000-model-ema.th"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WZBfqbPQuyu",
        "outputId": "3c67efa3-b159-4660-b714-1531ebb2a99e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: missing destination file operand after '/content/brain-decoding/vdvae/ffhq1024-iter-1700000-model-ema.th'\n",
            "Try 'mv --help' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq1024-iter-1700000-model.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "ajNHl8NlQv0-"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/ffhq1024-iter-1700000-opt.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "Nrh_Ep7YQwUB"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>CIFAR-10 </h3>"
      ],
      "metadata": {
        "id": "VAFpgaOH5FTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "bash /content/brain-decoding/vdvae/setup_cifar10.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfswITa5Q06j",
        "outputId": "33589232-b986-4e63-c680-a99711310111"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 17:57:11--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  21.4MB/s    in 8.2s    \n",
            "\n",
            "2023-10-27 17:57:20 (19.7 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MsRv4QAkrzEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# 39M parameters, trained for ~1M iterations with early stopping (a little less than a week on 2 GPUs)\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed0-iter-900000-model-ema.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed1-iter-1050000-model-ema.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed2-iter-650000-model-ema.th\n",
        "wget https://openaipublic.blob.core.windows.net/very-deep-vaes-assets/vdvae-assets-2/cifar10-seed3-iter-1050000-model-ema.th\n",
        "python train.py --hps cifar10 --restore_ema_path cifar10-seed0-iter-900000-model-ema.th --test_eval\n",
        "python train.py --hps cifar10 --restore_ema_path cifar10-seed1-iter-1050000-model-ema.th --test_eval\n",
        "python train.py --hps cifar10 --restore_ema_path cifar10-seed2-iter-650000-model-ema.th --test_eval\n",
        "python train.py --hps cifar10 --restore_ema_path cifar10-seed3-iter-1050000-model-ema.th --test_eval\n",
        "# seeds 0, 1, 2, 3 should give 2.879, 2.842, 2.898, 2.864 bits per dim, for an average of 2.87 bits per dim."
      ],
      "metadata": {
        "id": "-t_qLYIm5Fyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/cifar10-seed0-iter-900000-model-ema.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "Yr_66uBOn7rP"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/cifar10-seed1-iter-1050000-model-ema.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "93BsXEG8r_FS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/cifar10-seed2-iter-650000-model-ema.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "4U3RaW0NsAdJ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/vdvae/cifar10-seed3-iter-1050000-model-ema.th /content/brain-decoding/vdvae/model"
      ],
      "metadata": {
        "id": "XuZqCwjssBj4"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Move NSD files to repo dir</h3>"
      ],
      "metadata": {
        "id": "otkEhvi93iim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/content/brain-decoding/data/processed_data"
      ],
      "metadata": {
        "id": "Cc-6XDaf31VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_test_cap_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_test_cap_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_test_depth_stim_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_test_depth_stim_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_test_fmriavg_nsdgeneral_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_test_fmriavg_nsdgeneral_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_test_stim_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_test_stim_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_train_cap_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_train_cap_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_train_depth_stim_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_train_depth_stim_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_train_fmriavg_nsdgeneral_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_train_fmriavg_nsdgeneral_sub1.npy\n",
        "cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_train_stim_sub1.npy /content/brain-decoding/data/processed_data/subj01/nsd_train_stim_sub1.npy"
      ],
      "metadata": {
        "id": "zJQWdF5wuUB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45fd3f2-b46c-4552-ffeb-e409802bfd95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/brain-decoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypcZ_VbgsbF0",
        "outputId": "899ecd0a-b853-4228-ec5c-b865652d8a02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VDVAE Extract Features"
      ],
      "metadata": {
        "id": "2lQ2Jtq8Fz8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('vdvae')\n",
        "import torch\n",
        "import numpy as np\n",
        "#from mpi4py import MPI\n",
        "import socket\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "from utils import (logger,\n",
        "                   local_mpi_rank,\n",
        "                   mpi_size,\n",
        "                   maybe_download,\n",
        "                   mpi_rank)\n",
        "from data import mkdir_p\n",
        "from contextlib import contextmanager\n",
        "import torch.distributed as dist\n",
        "#from apex.optimizers import FusedAdam as AdamW\n",
        "from vae import VAE\n",
        "from torch.nn.parallel.distributed import DistributedDataParallel\n",
        "from train_helpers import restore_params\n",
        "from image_utils import *\n",
        "from model_utils import *\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import pickle"
      ],
      "metadata": {
        "id": "ai2IDTOCBkji"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "DC87A7_wMN7N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Argument Parser')\n",
        "parser.add_argument(\"-sub\", \"--sub\",help=\"Subject Number\",default=1)\n",
        "parser.add_argument(\"-bs\", \"--bs\",help=\"Batch Size\",default=30)\n",
        "args = parser.parse_args(\"\")"
      ],
      "metadata": {
        "id": "NuBYk2fGMbwq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub=int(args.sub)\n",
        "assert sub in [1,2,5,7]\n",
        "batch_size=int(args.bs)\n",
        "\n",
        "print('Libs imported')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XRgEqGjCG84",
        "outputId": "26340ff6-17f1-42d7-b80f-208a6b0dc076"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libs imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H = {'image_size': 64, 'image_channels': 3,'seed': 0, 'port': 29500, 'save_dir': './saved_models/test', 'data_root': './', 'desc': 'test', 'hparam_sets': 'imagenet64', 'restore_path': 'imagenet64-iter-1600000-model.th', 'restore_ema_path': 'vdvae/model/imagenet64-iter-1600000-model-ema.th', 'restore_log_path': 'imagenet64-iter-1600000-log.jsonl', 'restore_optimizer_path': 'imagenet64-iter-1600000-opt.th', 'dataset': 'imagenet64', 'ema_rate': 0.999, 'enc_blocks': '64x11,64d2,32x20,32d2,16x9,16d2,8x8,8d2,4x7,4d4,1x5', 'dec_blocks': '1x2,4m1,4x3,8m4,8x7,16m8,16x15,32m16,32x31,64m32,64x12', 'zdim': 16, 'width': 512, 'custom_width_str': '', 'bottleneck_multiple': 0.25, 'no_bias_above': 64, 'scale_encblock': False, 'test_eval': True, 'warmup_iters': 100, 'num_mixtures': 10, 'grad_clip': 220.0, 'skip_threshold': 380.0, 'lr': 0.00015, 'lr_prior': 0.00015, 'wd': 0.01, 'wd_prior': 0.0, 'num_epochs': 10000, 'n_batch': 4, 'adam_beta1': 0.9, 'adam_beta2': 0.9, 'temperature': 1.0, 'iters_per_ckpt': 25000, 'iters_per_print': 1000, 'iters_per_save': 10000, 'iters_per_images': 10000, 'epochs_per_eval': 1, 'epochs_per_probe': None, 'epochs_per_eval_save': 1, 'num_images_visualize': 8, 'num_variables_visualize': 6, 'num_temperatures_visualize': 3, 'mpi_size': 1, 'local_rank': 0, 'rank': 0, 'logdir': './saved_models/test/log'}\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "H = dotdict(H)\n",
        "\n",
        "H, preprocess_fn = set_up_data(H)\n",
        "\n",
        "print('Models Loading')\n",
        "ema_vae = load_vaes(H)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZA0MnxlC6nV",
        "outputId": "1bd9a5fd-6d9d-413a-cc44-4abdce4f7d48"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models Loading\n",
            "Restoring ema vae from vdvae/model/imagenet64-iter-1600000-model-ema.th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class batch_generator_external_images(Dataset):\n",
        "\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.im = np.load(data_path).astype(np.uint8)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img = Image.fromarray(self.im[idx])\n",
        "        img = T.functional.resize(img,(64,64))\n",
        "        img = torch.tensor(np.array(img)).float()\n",
        "        #img = img/255\n",
        "        #img = img*2 - 1\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return  len(self.im)"
      ],
      "metadata": {
        "id": "2d972qAXDgJ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'data/processed_data/subj{:02d}/nsd_train_stim_sub{}.npy'.format(sub,sub)\n",
        "train_images = batch_generator_external_images(data_path = image_path)\n",
        "\n",
        "image_path = 'data/processed_data/subj{:02d}/nsd_test_stim_sub{}.npy'.format(sub,sub)\n",
        "test_images = batch_generator_external_images(data_path = image_path)\n",
        "\n",
        "trainloader = DataLoader(train_images,batch_size,shuffle=False)\n",
        "testloader = DataLoader(test_images,batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "cxGLTp_2Dkca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#expand error message\n",
        "%tb"
      ],
      "metadata": {
        "id": "bc2xU2tUTYwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Latents </h3>"
      ],
      "metadata": {
        "id": "s9QVGkRWD9Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num_latents = 41\n",
        "num_latents = 41\n",
        "test_latents = []"
      ],
      "metadata": {
        "id": "jMHs8qbeD6SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,x in enumerate(testloader):\n",
        "  data_input, target = preprocess_fn(x)\n",
        "  with torch.no_grad():\n",
        "        print(i*batch_size)\n",
        "        activations = ema_vae.encoder.forward(data_input)\n",
        "        px_z, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "        #recons = ema_vae.decoder.out_net.sample(px_z)\n",
        "        batch_latent = []\n",
        "        for i in range(num_latents):\n",
        "            #test_latents[i].append(stats[i]['z'].cpu().numpy())\n",
        "            batch_latent.append(stats[i]['z'].cpu().numpy().reshape(len(data_input),-1))\n",
        "        test_latents.append(np.hstack(batch_latent))\n",
        "\n",
        "test_latents = np.concatenate(test_latents)"
      ],
      "metadata": {
        "id": "r3Xc40KvEEww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_latents = []\n",
        "for i,x in enumerate(trainloader):\n",
        "  data_input, target = preprocess_fn(x)\n",
        "  with torch.no_grad():\n",
        "        print(i*batch_size)\n",
        "        activations = ema_vae.encoder.forward(data_input)\n",
        "        px_z, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "        #recons = ema_vae.decoder.out_net.sample(px_z)\n",
        "        batch_latent = []\n",
        "        for i in range(num_latents):\n",
        "            batch_latent.append(stats[i]['z'].cpu().numpy().reshape(len(data_input),-1))\n",
        "        train_latents.append(np.hstack(batch_latent))\n",
        "train_latents = np.concatenate(train_latents)\n",
        "\n",
        "np.savez(\"data/extracted_features/subj{:02d}/nsd_vdvae_features_{}l.npz\".format(sub,num_latents),train_latents=train_latents,test_latents=test_latents)\n",
        "\n"
      ],
      "metadata": {
        "id": "y3tT8Z2fEMk5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}