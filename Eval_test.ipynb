{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "Ah0psYYxYjHh",
        "jYrbng6wbBkZ",
        "oZD4uKeyqDDA",
        "JDmMJj5CPY4h",
        "PQKBBbZEm97c",
        "dWwBXsyFq8Cb",
        "Djg4wqSKwohx"
      ],
      "mount_file_id": "1L-V7IZudYJ_aHpw3SE9bA0wOOt1XmLCd",
      "authorship_tag": "ABX9TyMdel5K5MEmsDZ5OKzBZmtW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichiyan/brain-decoding/blob/master/Eval_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Inception Score: “How realistic is this image relative to the pretrained model?”\n",
        "\n",
        "* FID Score: “How similar is this group of images relative to this other group of images?”\n",
        "\n",
        "* LPIPS Score: “Did the structure of this patch change, and by how much?”\n",
        "\n",
        "* SSIM, MSSIM, PSNR: “How noisy is this generated image as compared to the ground truth image?”"
      ],
      "metadata": {
        "id": "3BZQMxoH-Duv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up installation"
      ],
      "metadata": {
        "id": "KJzHREtqax3C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_AkIIjCiFGs",
        "outputId": "3e399c7b-9b78-412e-d29f-f82637d927bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain-decoding'...\n",
            "remote: Enumerating objects: 513, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 513 (delta 7), reused 0 (delta 0), pack-reused 497\u001b[K\n",
            "Receiving objects: 100% (513/513), 507.71 MiB | 13.49 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "Updating files: 100% (439/439), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ichiyan/brain-decoding.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/brain-decoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MKfkuxuH3AX",
        "outputId": "70aef6c0-8a4d-4602-8033-09815043a55b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IAGmOY1H0IV",
        "outputId": "288773ef-1e6f-4014-9cc7-fd0fcb2240d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating 94c1638..0ea2b40\n",
            "Fast-forward\n",
            " scripts/latest/eval_extract_features.py   | 146 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " scripts/latest/evaluate_reconstruction.py |  93 \u001b[32m++++++++++++++++++++++++++++++++\u001b[m\n",
            " scripts/latest/save_test_images.py        |  16 \u001b[32m++++++\u001b[m\n",
            " 3 files changed, 255 insertions(+)\n",
            " create mode 100644 scripts/latest/eval_extract_features.py\n",
            " create mode 100644 scripts/latest/evaluate_reconstruction.py\n",
            " create mode 100644 scripts/latest/save_test_images.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mv /content/brain-decoding/scripts/latest/evaluate_reconstruction.py /content"
      ],
      "metadata": {
        "id": "r7N_hu5YEJlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8NfnUBWjhUa",
        "outputId": "044c1834-c1ed-46a1-fb33-6cf75f54b367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics torch torch-fidelity ipyplot torchvision webdataset clip clip-retrieval"
      ],
      "metadata": {
        "id": "mThomAAYQMgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip-retrieval"
      ],
      "metadata": {
        "id": "bE_YZme5nqWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "2XAF-xDhGdjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as ImagePIL\n",
        "import os, sys\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cux1LWgNGh22"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "BJCGEfIZNLb0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import scipy.io as spio\n",
        "import nibabel as nib\n",
        "import torchvision\n",
        "import torchvision.models as tvmodels\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import clip\n",
        "\n",
        "import skimage.io as sio\n",
        "from skimage import data, img_as_float\n",
        "from skimage.transform import resize as imresize\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import scipy as sp"
      ],
      "metadata": {
        "id": "mmOz62y1Lixg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "LEMNrXgXMEBw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()"
      ],
      "metadata": {
        "id": "EUyNc-uPlEEJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert imgs to npy"
      ],
      "metadata": {
        "id": "Ah0psYYxYjHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Converts all images in a directory to '.npy' format.\n",
        "Use np.save and np.load to save and load the images.\n",
        "Use it for training your neural networks in ML/DL projects.\n",
        "'''\n",
        "\n",
        "# Path to image directory\n",
        "path = \"/content/drive/MyDrive/brain_decoding/data/reconstructed/425_stim_53l/\"\n",
        "dirs = os.listdir( path )\n",
        "dirs.sort()\n",
        "x_train=[]\n",
        "\n",
        "def load_dataset():\n",
        "    # Append images to a list\n",
        "    for item in dirs:\n",
        "      # print(os.path.isfile(path+item))\n",
        "      if os.path.isfile(path+item):\n",
        "        im = pImage.open(path+item).convert(\"RGB\")\n",
        "        im = np.array(im)\n",
        "        x_train.append(im)\n",
        "    # print(x_train)"
      ],
      "metadata": {
        "id": "ayHZ1SHfAOze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dataset()\n",
        "\n",
        "# Convert and save the list of images in '.npy' format\n",
        "imgset=np.array(x_train)\n",
        "np.save(\"425recons.npy\",imgset)"
      ],
      "metadata": {
        "id": "8SLb7Pi0E9Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub=1"
      ],
      "metadata": {
        "id": "i4jMUyclBjfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recon_imgs = np.load('/content/recons.npy'.format(sub,sub)).astype(np.uint8)"
      ],
      "metadata": {
        "id": "g2TFjcfjacyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recon_imgs1 = np.load('/content/425recons.npy'.format(sub,sub)).astype(np.uint8)"
      ],
      "metadata": {
        "id": "sZ431q-4BJGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(recon_imgs1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22r9gld-hEx0",
        "outputId": "3886f25f-e099-4875-af4c-6f5e06756d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(982, 425, 425, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recon_imgs.shape)\n",
        "print(recon_imgs1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNFWhWKEBHkh",
        "outputId": "0202ee89-2ea5-4cd4-e428-59b77efbbb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(982, 512, 512, 3)\n",
            "(149, 425, 425, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ground = ImagePIL.fromarray(gt_stim[ndx])"
      ],
      "metadata": {
        "id": "otq2lZA_A4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/425recons.npy /content/drive/MyDrive/brain_decoding/data/reconstructed/recons425.npy"
      ],
      "metadata": {
        "id": "JUn_b8YGGlHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to numpy"
      ],
      "metadata": {
        "id": "pMx7eU_0YHol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show image"
      ],
      "metadata": {
        "id": "jYrbng6wbBkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = 1\n",
        "ndx=115\n",
        "layerA = 53\n",
        "layerB = 41"
      ],
      "metadata": {
        "id": "8kOGJHg4fSTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_stim = np.load('/content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_test_stim_sub1.npy'.format(sub,sub)).astype(np.uint8)\n",
        "recons_imgs = np.load('/content/drive/MyDrive/brain_decoding/data/reconstructed/53l/recons.npy'.format(sub,sub)).astype(np.uint8)\n",
        "recons425_imgs = np.load('/content/drive/MyDrive/brain_decoding/data/reconstructed/recons425.npy'.format(sub,sub)).astype(np.uint8)"
      ],
      "metadata": {
        "id": "FFVkjeBnesfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gt_stim[ndx].shape)\n",
        "print(recons_imgs[ndx].shape)\n",
        "print(recons425_imgs[ndx].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im6mRYDAe8d5",
        "outputId": "5c2ad9a6-89d4-49ee-e06c-32156eddf3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(425, 425, 3)\n",
            "(512, 512, 3)\n",
            "(425, 425, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# img1 = Image('/content/drive/MyDrive/brain_decoding/data/reconstructed/{}l/{}.png'.format(layerA, ndx), width=300)\n",
        "#img2 = Image('/content/drive/MyDrive/brain_decoding/data/reconstructed/{}l/{}.png'.format(layerB, ndx), width=300)"
      ],
      "metadata": {
        "id": "vbEde8Lei2ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1 = ImagePIL.open('/content/drive/MyDrive/brain_decoding/data/reconstructed/{}l/{}.png'.format(layerA, ndx))\n",
        "img2 = ImagePIL.open('/content/drive/MyDrive/brain_decoding/data/reconstructed/{}l/{}.png'.format(layerB, ndx))\n",
        "ground = ImagePIL.fromarray(gt_stim[ndx])\n",
        "img3 = ImagePIL.fromarray(recons425_imgs[ndx])"
      ],
      "metadata": {
        "id": "hdagLe-ZlPTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ground Truth\")\n",
        "display(ground)\n",
        "print(layerA,\" Layers\")\n",
        "display(img1)\n",
        "print(layerB,\" Layers\")\n",
        "display(img2)"
      ],
      "metadata": {
        "id": "7MFWNoC5bRaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSIM\n",
        "\n",
        "*   preds – estimated image\n",
        "*   target – ground truth image\n",
        "\n",
        "[SSIM doc](https://torchmetrics.readthedocs.io/en/stable/image/structural_similarity.html)"
      ],
      "metadata": {
        "id": "oZD4uKeyqDDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access images"
      ],
      "metadata": {
        "id": "WT5yCO7fNjou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "DguFiFdsove4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.from_numpy(gt_stim)\n",
        "type(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWECVSeMkUxY",
        "outputId": "c2eec839-a554-4c0f-aa11-47a96c282edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preds = torch.rand([3, 3, 256, 256])\n",
        "# target = preds * 0.75\n",
        "preds = torch.from_numpy(recons425_imgs)\n",
        "# transform(img3).unsqueeze(0)\n",
        "target = torch.from_numpy(gt_stim)\n",
        "# transform(ground).unsqueeze(0)\n",
        "print(type(preds))\n",
        "print(preds.shape)\n",
        "print(\"ground: \", target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYjc81urIzxS",
        "outputId": "798e5684-2749-4bfa-b1ca-f6e3080f8bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([982, 425, 425, 3])\n",
            "ground:  torch.Size([982, 425, 425, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds[0]"
      ],
      "metadata": {
        "id": "eFl8G0szSki4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call SSIM"
      ],
      "metadata": {
        "id": "YM6fZ_c5NmA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
        "ssim(preds, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "ep32TqCFNgnM",
        "outputId": "d289f73a-9ffa-4069-bbde-ae04be08616f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-6d521ce6267c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructuralSimilarityIndexMeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_full_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_reduce_state_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36m_forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# calculate batch state and compute batch value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mbatch_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m                             \u001b[0;34m\" device corresponds to the device of the input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                         ) from err\n\u001b[0;32m--> 467\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_on_cpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/metric.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m\"Expected all tensors to be on\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/image/ssim.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"Update state with predictions and targets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssim_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         similarity_pack = _ssim_update(\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchmetrics/functional/image/ssim.py\u001b[0m in \u001b[0;36m_ssim_update\u001b[0;34m(preds, target, gaussian_kernel, sigma, kernel_size, data_range, k1, k2, return_full_image, return_contrast_sensitivity)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgauss_kernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reflect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reflect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgaussian_kernel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (5, 5) at dimension 3 of input [982, 425, 425, 3]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FID"
      ],
      "metadata": {
        "id": "JDmMJj5CPY4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "_ = torch.manual_seed(123)\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "import torch_fidelity\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "WBBGmTg2PfQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_fidelity.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WOtZReK2VzhF",
        "outputId": "2f3f1f0e-f576-4b9f-b983-913de57276d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fid = FrechetInceptionDistance(feature=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bB3ldlIQ1IG",
        "outputId": "8ff82476-9257-4388-ca65-ebddbe3a52d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 388MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ground = tf.data.Dataset.from_tensor_slices(gt_stim)\n",
        "dataset_ground"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWSYL8n7mEFJ",
        "outputId": "98b792fc-f276-458c-f6e9-9ae06e739085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(425, 425, 3), dtype=tf.uint8, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_recon = tf.data.Dataset.from_tensor_slices(recons425_imgs)\n",
        "dataset_recon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WnXH9EUmR0R",
        "outputId": "8d9ea9a0-cb24-4726-fd2a-d90cc08d0cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(425, 425, 3), dtype=tf.uint8, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate two slightly overlapping image intensity distributions\n",
        "imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n",
        "imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)"
      ],
      "metadata": {
        "id": "zqz7Gdc9Pwl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid.update(imgs_dist1, real=True)\n",
        "fid.update(imgs_dist2, real=False)\n",
        "fid.compute()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSnldzvyPurE",
        "outputId": "f3aa1f1d-39d7-4fa6-b325-9dcb222d1b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(12.7202)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_dict = torch_fidelity.calculate_metrics(\n",
        "    input1=dataset_recon,\n",
        "    input2=dataset_ground,\n",
        "    cuda=True,\n",
        "    isc=True,\n",
        "    fid=True,\n",
        "    kid=True,\n",
        "    prc=True,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "VdlrU92MZakP",
        "outputId": "5411426e-136b-4849-e202-a64637e3affe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-05ec8841310a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m metrics_dict = torch_fidelity.calculate_metrics(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_recon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_ground\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0misc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/metrics.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mvprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Extracting features from input1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mfeaturesdict_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_featuresdict_from_input_id_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mfeaturesdict_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/utils.py\u001b[0m in \u001b[0;36mextract_featuresdict_from_input_id_cached\u001b[0;34m(input_id, feat_extractor, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         )\n\u001b[1;32m    371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfeaturesdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_recompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeaturesdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/utils.py\u001b[0m in \u001b[0;36mfn_recompute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_recompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mextract_featuresdict_from_input_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcacheable_input_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/utils.py\u001b[0m in \u001b[0;36mextract_featuresdict_from_input_id\u001b[0;34m(input_id, feat_extractor, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0mrng_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rng_seed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'verbose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_input_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0msave_cpu_ram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'save_cpu_ram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/utils.py\u001b[0m in \u001b[0;36mprepare_input_from_id\u001b[0;34m(input_id, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_input_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0minput_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_input_descriptor_from_input_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_input_from_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/utils.py\u001b[0m in \u001b[0;36mprepare_input_from_descriptor\u001b[0;34m(input_desc, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mbad_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     vassert(\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0mbad_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;34mf'Input descriptor \"input\" field can be either an instance of Dataset, GenerativeModelBase class, or a string, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_fidelity/helpers.py\u001b[0m in \u001b[0;36mvassert\u001b[0;34m(truecond, message)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvassert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtruecond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input descriptor \"input\" field can be either an instance of Dataset, GenerativeModelBase class, or a string, such as a path to a name of a registered dataset (cifar10-train, cifar10-val, stl10-train, stl10-test, stl10-unlabeled), a directory with file samples, or a path to an ONNX or PTH (JIT) module"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FID [pytorch]"
      ],
      "metadata": {
        "id": "PQKBBbZEm97c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are under the required 2048 images, select the dimensionality of features to use with the flag --dims N, where N is the dimensionality of features. The choices are:\n",
        "\n",
        "* 64: first max pooling features\n",
        "* 192: second max pooling features\n",
        "* 768: pre-aux classifier features\n",
        "* 2048: final average pooling features (this is the default)"
      ],
      "metadata": {
        "id": "GzfBMhNvxSk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "id": "OewMrQW-m8IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytorch_fid /content/drive/MyDrive/brain_decoding/data/reconstructed/41l/stim_41l /content/drive/MyDrive/brain_decoding/data/reconstructed/53l/stim_53l --dims 768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMbcjYV7nFxg",
        "outputId": "1a0ddd70-6f05-46c0-8cf3-b6e4902c6ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 20/20 [00:06<00:00,  3.27it/s]\n",
            "100% 20/20 [00:06<00:00,  3.31it/s]\n",
            "FID:  0.09330198096745956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BrainDiffuser"
      ],
      "metadata": {
        "id": "owXsNjojqEwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span style=\"background:Green\">\n",
        "Quantitative Evaluation\n",
        "Although results are expected to be similar, it may vary because of variations at reconstruction\n",
        "\n",
        "\n",
        "*   Save test images to directory ```python scripts/save_test_images.py```\n",
        "*  Extract evaluation features for test images using\n",
        "```\n",
        "# python scripts/eval_extract_features.py -sub 0\n",
        "```\n",
        "*   Extract evaluation features for reconstructed images of any subject using ```python scripts/eval_extract_features.py -sub x```\n",
        "*   Obtain quantitative metric results for each subject using ```python scripts/evaluate_reconstruction.py -sub x```\n",
        "</span>"
      ],
      "metadata": {
        "id": "TAvIvZrG5rbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/brain-decoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ya7nokusFnb",
        "outputId": "4482d98c-5b3b-4ba8-b201-27cd92ae6718"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>From npy to images</h2>"
      ],
      "metadata": {
        "id": "fiYdoRpiHk7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.load('/content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_test_stim_sub1.npy')\n",
        "#PATH WHERE IMAGES GET SAVED TO\n",
        "test_images_dir = '/content/brain-decoding/data/nsddata_stimuli/stimuli'\n",
        "\n",
        "if not os.path.exists(test_images_dir):\n",
        "   os.makedirs(test_images_dir)\n",
        "for i in range(len(images)):\n",
        "    im = ImagePIL.fromarray(images[i].astype(np.uint8))\n",
        "    im.save('{}/{}.png'.format(test_images_dir,i))"
      ],
      "metadata": {
        "id": "Pb6rA2jrHjt8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Extract Features</h2>"
      ],
      "metadata": {
        "id": "RMlNxLg3IMKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12JT7mBqQQJP",
        "outputId": "a1e4ec0e-c665-4ec8-9186-4d65eef6e368"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes made\n",
        "* device=0 [since there's only 1 gpu]\n",
        "* replace `images_dir` & `feats_dir`"
      ],
      "metadata": {
        "id": "3iaaGKZkR3nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/brain-decoding/scripts/latest/eval_extract_features.py -sub 0"
      ],
      "metadata": {
        "id": "0doq8GZWIRmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
      ],
      "metadata": {
        "id": "t6fntZDGnLyQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_available_gpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VOYdjzcnOrI",
        "outputId": "5b856e7d-cc58-4a43-ca46-2bb01d11d1de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/device:GPU:0']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Evaluate Reconstruction</h2>"
      ],
      "metadata": {
        "id": "-wgnDUaPICoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* change `images_dir` & `feats_dir` at line50\n",
        "* double check the paths used for `gen_image` & `gt_image` at line 83"
      ],
      "metadata": {
        "id": "J1sfG659NHHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/brain-decoding/scripts/latest/editted_evaluate_reconstruction.py -sub 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi2b8E7_qCLt",
        "outputId": "9393c207-9804-4152-f236-3eb2fa646306"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding/scripts/latest/editted_evaluate_reconstruction.py:91: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  ssim_res = ssim(gen_image, gt_image, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0)\n",
            "PixCorr: 0.031211689812505133\n",
            "SSIM: 0.34881656110681275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/evaluate_reconstruction.py /content/drive/MyDrive/brain_decoding/editted_evaluate_reconstruction.py"
      ],
      "metadata": {
        "id": "bRPOFwR_bN4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MindEye\n"
      ],
      "metadata": {
        "id": "dWwBXsyFq8Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating Reconstructions\n",
        "After you have saved a .pt file from running `Reconstructions.py` or `Retrievals.py`, you can use `Reconstruction_Metrics.py` to evaluate reconstructed images using the same low- and high-level image metrics used in the paper.\n",
        "\n",
        "* Set `recon_path` to the name of the file in \"fMRI-reconstruction-NSD/src\" that was output from *Reconstructions.py* (should be `{model_name}_recons_img2img{img2img_strength}_{recons_per_sample}samples.pt`).\n",
        "* Alternatively, to evaluate LAION-5B retrievals, you can replace recon_path with the name of the .pt file output from *Retrievals.py* (should be `{model_name}_laion_retrievals_top16.pt`).\n",
        "* Set `all_images_path` to the all_images.pt file in \"fMRI-reconstruction-NSD/src\" that was output from either *Reconstructions.py* or *Retrievals.py* (should be `all_images.pt`).\n",
        "\n",
        "```\n",
        "$ python Reconstruction_Metrics.py --help\n",
        "```\n",
        "\n",
        "```\n",
        "usage: Reconstruction_Metrics.py [-h] [--recon_path RECON_PATH]\n",
        "                                 [--all_images_path ALL_IMAGES_PATH]\n",
        "\n",
        "Model Training Configuration\n",
        "\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --recon_path RECON_PATH\n",
        "                        path to reconstructed/retrieved outputs\n",
        "  --all_images_path ALL_IMAGES_PATH\n",
        "                        path to ground truth outputs\n",
        "```"
      ],
      "metadata": {
        "id": "BRTX8EC5AasV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch dalle2_pytorch typing-extensions"
      ],
      "metadata": {
        "id": "m3NKgwFA2mIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install typing-extensions --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHDjtGbj4bLu",
        "outputId": "0e841244-a5e6-43e5-e707-5b6de5822e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "id": "-i4KOwWW6Zx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/clip_client.py /content/brain-decoding/mindeye/src/clip_client.py"
      ],
      "metadata": {
        "id": "eFI-9XbJtOje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/brain-decoding/mindeye/src')"
      ],
      "metadata": {
        "id": "Fpf_6xZpssWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip_retrieval"
      ],
      "metadata": {
        "id": "NlcbINXUOqyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieZP4M7_OaT5",
        "outputId": "e481860d-e085-4ab4-fc60-3522a890f9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import webdataset, sys\n",
        "\n",
        "import utils"
      ],
      "metadata": {
        "id": "XYSdVfGtdDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/brain-decoding/mindeye/src/Reconstruction_Metrics.py --recon_path '/content/drive/MyDrive/brain_decoding/data/reconstructed/recons425.npy' --all_images_path '/content/drive/MyDrive/brain_decoding/data/fMRI/NSD/nsd_train_stim_sub1.npy'"
      ],
      "metadata": {
        "id": "Xp_Alzmp8fui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b005ea30-d33b-4db3-ca3e-9c995943a88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "ViT-L/14 cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/brain-decoding/mindeye/src/Reconstruction_Metrics.py\", line 89, in <module>\n",
            "    all_brain_recons = torch.load(f'{recon_path}')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1028, in load\n",
            "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1246, in _legacy_load\n",
            "    magic_number = pickle_module.load(f, **pickle_load_args)\n",
            "_pickle.UnpicklingError: STACK_GLOBAL requires str\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "Djg4wqSKwohx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_drive_path = os.path.join('/content/drive/MyDrive/brain_decoding/data/mindeye low-level/ckpt/imgds.txt')\n"
      ],
      "metadata": {
        "id": "bQmJcQLhtEPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([0, 1, 2, 3, 4])\n",
        "torch.save(x, ckpt_drive_path)"
      ],
      "metadata": {
        "id": "GGjf4V2JubC4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}