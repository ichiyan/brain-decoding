{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, '../mindeye/src/')\n",
    "\n",
    "# from pipeline_stable_diffusion_xl_adapter import StableDiffusionXLAdapterPipeline\n",
    "from diffusers import T2IAdapter,  EulerDiscreteScheduler, AutoencoderKL, MultiAdapter, StableDiffusionAdapterPipeline\n",
    "from diffusers.utils import load_image\n",
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "# from transformers import AutoProcessor, SafetyChecker\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# import utils \n",
    "from models import Clipper, BrainNetwork, BrainDiffusionPriorOld \n",
    "\n",
    "import clip \n",
    "from clip_client import ClipClient, Modality\n",
    "import urllib\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 1\n",
    "#for vdvae depth initial image\n",
    "vdvae_num_layers = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_stim_test = np.load('../data/processed_data/subj{:02d}/nsd_test_stim_sub1.npy'.format(subj,subj)).astype(np.uint8)\n",
    "caps_test = np.load('../data/processed_data/subj{:02d}/nsd_test_cap_sub{}.npy'.format(subj,subj))\n",
    "fmri_test = np.load('../data/processed_data/subj{:02d}/nsd_test_fmriavg_nsdgeneral_sub{}.npy'.format(subj,subj))\n",
    "pred_cliptext = np.load('../data/predicted_features/subj{:02d}/nsd_compel_cliptext_predtest_nsdgeneral.npy'.format(subj))\n",
    "pred_cliptext = torch.from_numpy(pred_cliptext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load BrainNetwork (voxel2clip) and Diffusion Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj 1 num_voxels 15724\n"
     ]
    }
   ],
   "source": [
    "if subj == 1:\n",
    "    num_voxels = 15724\n",
    "elif subj == 2:\n",
    "    num_voxels = 14278\n",
    "elif subj == 3:\n",
    "    num_voxels = 15226\n",
    "elif subj == 4:\n",
    "    num_voxels = 13153\n",
    "elif subj == 5:\n",
    "    num_voxels = 13039\n",
    "elif subj == 6:\n",
    "    num_voxels = 17907\n",
    "elif subj == 7:\n",
    "    num_voxels = 12682\n",
    "elif subj == 8:\n",
    "    num_voxels = 14386\n",
    "print(\"subj\",subj,\"num_voxels\",num_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n",
      "ckpt_path ../mindeye/train_logs/prior_1x768_final_subj01_bimixco_softclip_byol/last.pth\n",
      "EPOCH:  299\n"
     ]
    }
   ],
   "source": [
    "# CLS model\n",
    "out_dim = 768\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=False, norm_embs=False, device=device)\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip.requires_grad_(False)\n",
    "voxel2clip.eval()\n",
    "\n",
    "diffusion_prior = BrainDiffusionPriorOld.from_pretrained(\n",
    "    # kwargs for DiffusionPriorNetwork\n",
    "    dict(),\n",
    "    # kwargs for DiffusionNetwork\n",
    "    dict(\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=1000,\n",
    "        voxel2clip=voxel2clip,\n",
    "    ),\n",
    "    voxel2clip_path=None,\n",
    "    ckpt_dir='../mindeye/checkpoints',\n",
    ")\n",
    "\n",
    "model_name = \"prior_1x768_final_subj01_bimixco_softclip_byol\"\n",
    "outdir = f'../mindeye/train_logs/{model_name}/'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior.eval().to(device)\n",
    "diffusion_priors = [diffusion_prior]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to get CLIP embeddings from fMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embeddings(voxels, diffusion_prior, recons_per_sample=1, seed=0):\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(seed)\n",
    "    brain_clip_embeddings0, proj_embeddings = diffusion_prior.voxel2clip(voxels.to(device).float())\n",
    "    # brain_clip_embeddings0 = brain_clip_embeddings0.view(len(voxels),-1,768)\n",
    "    brain_clip_embeddings0 = brain_clip_embeddings0.view(-1,768)\n",
    "    brain_clip_embeddings0 = brain_clip_embeddings0.repeat(recons_per_sample, 1)\n",
    "    brain_clip_embeddings = diffusion_prior.p_sample_loop(brain_clip_embeddings0.shape,\n",
    "                            text_cond = dict(text_embed = brain_clip_embeddings0),\n",
    "                            cond_scale = 1., timesteps = 1000, #1000 timesteps used from nousr pretraining\n",
    "                            generator=generator)\n",
    "    brain_clip_embeddings = brain_clip_embeddings.unsqueeze(1) #(1,1,768)\n",
    "\n",
    "    return brain_clip_embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load CLIP Client and ClIP retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14\", device=\"cpu\", jit=False)\n",
    "\n",
    "client = ClipClient(\n",
    "    url=\"https://knn.laion.ai/knn-service\",\n",
    "    indice_name=\"laion5B-L-14\",\n",
    "    aesthetic_score=9,\n",
    "    aesthetic_weight=0.5,\n",
    "    modality=Modality.IMAGE,\n",
    "    num_images=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_result(result):\n",
    "    id, caption, url, similarity = result[\"id\"], result[\"caption\"], result[\"url\"], result[\"similarity\"]\n",
    "    print(f\"id: {id}\")\n",
    "    print(f\"caption: {caption}\")\n",
    "    print(f\"url: {url}\")\n",
    "    print(f\"similarity: {similarity}\")\n",
    "    display(Image(url=url, unconfined=True))\n",
    "\n",
    "def download_image(url):\n",
    "    urllib_request = urllib.request.Request(\n",
    "        url,\n",
    "        data=None,\n",
    "        headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"},\n",
    "    )\n",
    "    with urllib.request.urlopen(urllib_request, timeout=10) as r:\n",
    "        img_stream = io.BytesIO(r.read())\n",
    "    return img_stream\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def get_text_emb(text):\n",
    "    with torch.no_grad():\n",
    "        text_emb = model.encode_text(clip.tokenize([text], truncate=True).to(\"cpu\"))\n",
    "        text_emb /= text_emb.norm(dim=-1, keepdim=True)\n",
    "        text_emb = text_emb.cpu().detach().numpy().astype(\"float32\")[0]\n",
    "    return text_emb\n",
    "\n",
    "\n",
    "def get_image_emb(image_url):\n",
    "    with torch.no_grad():\n",
    "        image = Image.open(download_image(image_url))\n",
    "        image_emb = model.encode_image(preprocess(image).unsqueeze(0).to(\"cpu\"))\n",
    "        image_emb /= image_emb.norm(dim=-1, keepdim=True)\n",
    "        image_emb = image_emb.cpu().detach().numpy().astype(\"float32\")[0]\n",
    "        return image_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Stable Diffusion with T2I Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters = MultiAdapter(\n",
    "    [\n",
    "        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_color_sd14v1\", torch_dtype=torch.float16),\n",
    "        T2IAdapter.from_pretrained(\"TencentARC/t2iadapter_depth_sd14v1\", torch_dtype=torch.float16),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "vae = AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema', torch_dtype=torch.float16)\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subjfolder=\"scheduler\")\n",
    "pipe = StableDiffusionAdapterPipeline.from_pretrained(\n",
    "    model_id, vae=vae, adapter=adapters, scheduler=scheduler, torch_dtype=torch.float16, variant=\"fp16\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_pallete(img):\n",
    "    img = img.resize((8,8))\n",
    "    color_pallete = img.resize((512, 512), resample=Image.Resampling.NEAREST)\n",
    "\n",
    "    return color_pallete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\"\n",
    "generator = torch.manual_seed(777) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ndx, voxels in enumerate(torch.from_numpy(fmri_test)):\n",
    "    print(ndx)\n",
    "\n",
    "    clip_embs = get_clip_embeddings(voxels, diffusion_prior)\n",
    "    query_res = client.query(embedding_input=clip_embs[0][0].tolist())\n",
    "    caps = query_res[0][\"caption\"]\n",
    "    print(\"prompt: \", caps)\n",
    "    print(\"retrieved embs similarity: \", query_res[0][\"similarity\"])\n",
    "\n",
    "    color_pallete = get_color_pallete(Image.open('../results/lvae/subj01/stim/{}.png'.format(ndx)))\n",
    "    depth_img = Image.open('../results/vdvae/subj01/depth_stim_{}l/{}.png'.format(vdvae_num_layers,ndx))\n",
    "\n",
    "    gen_images = pipe(\n",
    "    prompt = caps,\n",
    "    negative_prompt=negative_prompt,\n",
    "    image=[color_pallete, depth_img],\n",
    "    adapter_conditioning_scale=[0.9, 0.8],\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=5.5,  \n",
    "    generator=generator,\n",
    "    num_images_per_prompt=3,\n",
    "    ).images\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain-decoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
