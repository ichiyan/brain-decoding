{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMdycyWIyvGU"
      },
      "source": [
        "Notes:\n",
        "*   code to be added later to extraction in lvae colab\n",
        "*   run lvae colab notebook first since fmri is processed and saved there\n",
        "*   saving of ground truth images is also in lvae colab notebook\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVCKvBiKrBPk",
        "outputId": "16f8c2e3-c2d4-43a7-8fba-b98349736587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkuIx4ibrR3U",
        "outputId": "d036837b-95ca-4d8d-f4e1-8e481d21335a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain-decoding'...\n",
            "remote: Enumerating objects: 548, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 548 (delta 21), reused 26 (delta 9), pack-reused 497\u001b[K\n",
            "Receiving objects: 100% (548/548), 513.73 MiB | 19.77 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ichiyan/brain-decoding.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ZEygzKrTod",
        "outputId": "010948ff-8fd6-4317-dceb-2721a3f8a8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ],
      "source": [
        "cd /content/brain-decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVNC0pCUrcfE",
        "outputId": "378c9907-1d8a-422d-98dc-9bee6f8cde8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrWi-n85riEU"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcNOtBOMsuiJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U controlnet_aux==0.0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZya6-kAsyqv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate safetensors einops torchvision omegaconf torchtext ml-collections gdown webdataset braceexpand mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmafnDE2rnGA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMLc81irej_",
        "outputId": "187afe3b-bbbc-4fa9-f552-905503aa1a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding/scripts/latest\n"
          ]
        }
      ],
      "source": [
        "cd /content/brain-decoding/scripts/latest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LcyJbCibrf3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a9b694-500a-4ae2-bc53-7525f10b7aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# sys.path.append('vdvae')\n",
        "sys.path.insert(1, '../../vdvae/')\n",
        "import torch\n",
        "import numpy as np\n",
        "#from mpi4py import MPI\n",
        "import socket\n",
        "import subprocess\n",
        "from hps import Hyperparams, parse_args_and_update_hparams, add_vae_arguments\n",
        "from utils import (logger,\n",
        "                   local_mpi_rank,\n",
        "                   mpi_size,\n",
        "                   maybe_download,\n",
        "                   mpi_rank)\n",
        "from data import mkdir_p\n",
        "from contextlib import contextmanager\n",
        "import torch.distributed as dist\n",
        "#from apex.optimizers import FusedAdam as AdamW\n",
        "from vae import VAE\n",
        "from torch.nn.parallel.distributed import DistributedDataParallel\n",
        "from train_helpers import restore_params\n",
        "from image_utils import *\n",
        "from model_utils import *\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "import webdataset as wds\n",
        "import braceexpand\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from controlnet_aux.midas import MidasDetector\n",
        "\n",
        "# for debugging in colab\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "80pPMUKRrqJM"
      },
      "outputs": [],
      "source": [
        "subj_id = \"02\"\n",
        "sub = 2\n",
        "num_latents = 47\n",
        "alpha = 100\n",
        "\n",
        "# batch_size and test_batch_size must be 1 since midas_depth seems to only accept one image\n",
        "batch_size = 1\n",
        "test_batch_size = 1\n",
        "local_rank = 0\n",
        "num_devices = 1\n",
        "num_workers = 1\n",
        "seed = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLEOg-YBsZ3V",
        "outputId": "7b1b2439-69d2-4a7b-d8bc-33b0df171e07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbr3_z8Oruxc"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBTeeL2frxgJ",
        "outputId": "4e5f622a-3b0c-4f27-d1d3-ff6f44b5587a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepping train and validation dataloaders...\n"
          ]
        }
      ],
      "source": [
        "print('Prepping train and validation dataloaders...')\n",
        "num_train = 8859\n",
        "num_val = 982"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEwKM5irtlx",
        "outputId": "4f29bb0e-3277-4cd0-f8ac-e9945e5b8fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulling NSD webdataset data...\n"
          ]
        }
      ],
      "source": [
        "print('Pulling NSD webdataset data...')\n",
        "\n",
        "# urls should be local path\n",
        "train_url = f\"{{/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/train/train_subj{subj_id}_{{0..17}}.tar,/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/val/val_subj{subj_id}_0.tar}}\"\n",
        "val_url = f\"/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/test/test_subj{subj_id}_{{0..1}}.tar\"\n",
        "meta_url = f\"/fsx/proj-fmri/shared/natural-scenes-dataset/webdataset_avg_split/metadata_subj{subj_id}.json\"\n",
        "\n",
        "# mindeye code\n",
        "# train_url = f\"../data/train_subj01_\"+\"{0..2}.tar\"\n",
        "# val_url = f\"../data/val_subj01_0.tar\"\n",
        "# meta_url = f\"../data/metadata_subj01.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AoHipxM-rzH9"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(\n",
        "    batch_size,\n",
        "    image_var='images',\n",
        "    num_devices=None,\n",
        "    num_workers=None,\n",
        "    train_url=None,\n",
        "    val_url=None,\n",
        "    meta_url=None,\n",
        "    num_train=None,\n",
        "    num_val=None,\n",
        "    cache_dir=\"/tmp/wds-cache\",\n",
        "    seed=0,\n",
        "    voxels_key=\"nsdgeneral.npy\",\n",
        "    val_batch_size=None,\n",
        "    to_tuple=[\"voxels\", \"images\", \"trial\"],\n",
        "    local_rank=0,\n",
        "    world_size=1,\n",
        "    subj=1,\n",
        "):\n",
        "    print(\"Getting dataloaders...\")\n",
        "    assert image_var == 'images'\n",
        "\n",
        "    def my_split_by_node(urls):\n",
        "        return urls\n",
        "\n",
        "    train_url = list(braceexpand.braceexpand(train_url))\n",
        "    val_url = list(braceexpand.braceexpand(val_url))\n",
        "    if not os.path.exists(train_url[0]):\n",
        "        # we will default to downloading from huggingface urls if data_path does not exist\n",
        "        print(\"downloading NSD from huggingface...\")\n",
        "        os.makedirs(cache_dir,exist_ok=True)\n",
        "\n",
        "        # train_url, val_url, test_url = get_huggingface_urls(\"main\",subj)\n",
        "        train_url, test_url = get_huggingface_urls(\"main\",subj)\n",
        "        train_url = list(braceexpand.braceexpand(train_url))\n",
        "        # val_url = list(braceexpand.braceexpand(val_url))\n",
        "        test_url = list(braceexpand.braceexpand(test_url))\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        for url in tqdm(train_url):\n",
        "            destination = cache_dir + \"/\" + url.rsplit('/', 1)[-1]\n",
        "            print(f\"\\nDownloading {url} to {destination}...\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(destination, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "\n",
        "        # for url in tqdm(val_url):\n",
        "        #     destination = cache_dir + \"/\" + url.rsplit('/', 1)[-1]\n",
        "        #     print(f\"\\nDownloading {url} to {destination}...\")\n",
        "        #     response = requests.get(url)\n",
        "        #     response.raise_for_status()\n",
        "        #     with open(destination, 'wb') as file:\n",
        "        #         file.write(response.content)\n",
        "\n",
        "        for url in tqdm(test_url):\n",
        "            destination = cache_dir + \"/\" + url.rsplit('/', 1)[-1]\n",
        "            print(f\"\\nDownloading {url} to {destination}...\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(destination, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "\n",
        "    if num_devices is None:\n",
        "        num_devices = torch.cuda.device_count()\n",
        "\n",
        "    if num_workers is None:\n",
        "        num_workers = num_devices\n",
        "\n",
        "    if num_train is None:\n",
        "        metadata = json.load(open(meta_url))\n",
        "        num_train = metadata['totals']['train']\n",
        "    if num_val is None:\n",
        "        metadata = json.load(open(meta_url))\n",
        "        num_val = metadata['totals']['val']\n",
        "\n",
        "    if val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "\n",
        "    global_batch_size = batch_size * num_devices\n",
        "    num_batches = math.floor(num_train / global_batch_size)\n",
        "    num_worker_batches = math.floor(num_batches / num_workers)\n",
        "    if num_worker_batches == 0: num_worker_batches = 1\n",
        "\n",
        "    print(\"\\nnum_train\",num_train)\n",
        "    print(\"global_batch_size\",global_batch_size)\n",
        "    print(\"batch_size\",batch_size)\n",
        "    print(\"num_workers\",num_workers)\n",
        "    print(\"num_batches\",num_batches)\n",
        "    print(\"num_worker_batches\", num_worker_batches)\n",
        "\n",
        "    # train_url = train_url[local_rank:world_size]\n",
        "    # train_data = wds.WebDataset(train_url, resampled=True, cache_dir=cache_dir, nodesplitter=my_split_by_node)\\\n",
        "    #     .shuffle(500, initial=500, rng=random.Random(42))\\\n",
        "    #     .decode(\"torch\")\\\n",
        "    #     .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
        "    #     .to_tuple(*to_tuple)\\\n",
        "    #     .batched(batch_size, partial=True)\\\n",
        "    #     .with_epoch(num_worker_batches)\n",
        "\n",
        "    train_data = wds.WebDataset(train_url, resampled=False, cache_dir=cache_dir, nodesplitter=my_split_by_node)\\\n",
        "        .decode(\"torch\")\\\n",
        "        .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
        "        .to_tuple(*to_tuple)\\\n",
        "        .batched(batch_size, partial=False)\n",
        "\n",
        "    train_dl = torch.utils.data.DataLoader(train_data, batch_size=None, num_workers=1, shuffle=False)\n",
        "\n",
        "    # Validation\n",
        "    # should be deterministic, no shuffling!\n",
        "    num_batches = math.floor(num_val / global_batch_size)\n",
        "    num_worker_batches = math.floor(num_batches / num_workers)\n",
        "    if num_worker_batches == 0: num_worker_batches = 1\n",
        "\n",
        "    print(\"\\nnum_val\",num_val)\n",
        "    print(\"val_num_batches\",num_batches)\n",
        "    print(\"val_batch_size\",val_batch_size)\n",
        "\n",
        "    # val_data = wds.WebDataset(val_url, resampled=False, cache_dir=cache_dir, nodesplitter=my_split_by_node)\\\n",
        "    #     .decode(\"torch\")\\\n",
        "    #     .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
        "    #     .to_tuple(*to_tuple)\\\n",
        "    #     .batched(val_batch_size, partial=False)\n",
        "\n",
        "    # val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, num_workers=1, shuffle=False)\n",
        "\n",
        "    test_data = wds.WebDataset(test_url, resampled=False, cache_dir=cache_dir, nodesplitter=my_split_by_node)\\\n",
        "        .decode(\"torch\")\\\n",
        "        .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
        "        .to_tuple(*to_tuple)\\\n",
        "        .batched(val_batch_size, partial=False)\n",
        "\n",
        "    test_dl = torch.utils.data.DataLoader(test_data, batch_size=None, num_workers=1, shuffle=False)\n",
        "\n",
        "    return train_dl, test_dl, num_train, num_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Dm-jYZIsr2SG"
      },
      "outputs": [],
      "source": [
        "def get_huggingface_urls(commit='main',subj=1):\n",
        "    base_url = \"https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/\"\n",
        "    train_url = base_url + commit + f\"/webdataset_avg_split/train/train_subj0{subj}_\" + \"{0..17}.tar\"\n",
        "    # val_url = base_url + commit + f\"/webdataset_avg_split/val/val_subj0{subj}_0.tar\"\n",
        "    test_url = base_url + commit + f\"/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
        "    # return train_url, val_url, test_url\n",
        "    return train_url, test_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5JEyvvqr4Dk",
        "outputId": "443b8bf9-fba6-43c8-cb15-7afc5cf976ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data for subject 2...\n",
            "Getting dataloaders...\n",
            "downloading NSD from huggingface...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/18 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_0.tar to /tmp/wds-cache/train_subj02_0.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 1/18 [00:10<03:05, 10.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_1.tar to /tmp/wds-cache/train_subj02_1.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 2/18 [00:21<02:53, 10.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_2.tar to /tmp/wds-cache/train_subj02_2.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 3/18 [00:34<02:54, 11.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_3.tar to /tmp/wds-cache/train_subj02_3.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 4/18 [01:01<04:10, 17.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_4.tar to /tmp/wds-cache/train_subj02_4.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 5/18 [01:24<04:12, 19.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_5.tar to /tmp/wds-cache/train_subj02_5.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 6/18 [01:38<03:31, 17.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_6.tar to /tmp/wds-cache/train_subj02_6.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 7/18 [01:55<03:11, 17.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_7.tar to /tmp/wds-cache/train_subj02_7.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 8/18 [02:08<02:40, 16.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_8.tar to /tmp/wds-cache/train_subj02_8.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 9/18 [02:22<02:19, 15.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_9.tar to /tmp/wds-cache/train_subj02_9.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 10/18 [02:38<02:05, 15.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_10.tar to /tmp/wds-cache/train_subj02_10.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 11/18 [02:51<01:44, 14.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_11.tar to /tmp/wds-cache/train_subj02_11.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 12/18 [03:03<01:24, 14.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_12.tar to /tmp/wds-cache/train_subj02_12.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 13/18 [03:14<01:05, 13.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_13.tar to /tmp/wds-cache/train_subj02_13.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 14/18 [03:26<00:50, 12.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_14.tar to /tmp/wds-cache/train_subj02_14.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 15/18 [03:37<00:36, 12.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_15.tar to /tmp/wds-cache/train_subj02_15.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 16/18 [03:49<00:24, 12.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_16.tar to /tmp/wds-cache/train_subj02_16.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 17/18 [04:00<00:11, 11.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/train/train_subj02_17.tar to /tmp/wds-cache/train_subj02_17.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [04:03<00:00, 13.51s/it]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/test/test_subj02_0.tar to /tmp/wds-cache/test_subj02_0.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:11<00:11, 11.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/main/webdataset_avg_split/test/test_subj02_1.tar to /tmp/wds-cache/test_subj02_1.tar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:22<00:00, 11.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "num_train 8859\n",
            "global_batch_size 1\n",
            "batch_size 1\n",
            "num_workers 1\n",
            "num_batches 8859\n",
            "num_worker_batches 8859\n",
            "\n",
            "num_val 982\n",
            "val_num_batches 982\n",
            "val_batch_size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Fetching data for subject {sub}...\")\n",
        "train_dl, test_dl, num_train, num_test = get_dataloaders(\n",
        "    batch_size,\n",
        "    num_devices=num_devices,\n",
        "    num_workers=num_workers,\n",
        "    train_url=train_url,\n",
        "    val_url=val_url,\n",
        "    meta_url=meta_url,\n",
        "    # val_batch_size=max(16, batch_size),\n",
        "    val_batch_size=test_batch_size,\n",
        "    cache_dir='/tmp/wds-cache',\n",
        "    seed=seed+local_rank,\n",
        "    voxels_key='nsdgeneral.npy',\n",
        "    local_rank=local_rank,\n",
        "    num_train=num_train,\n",
        "    num_val=num_val,\n",
        "    subj=sub\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrHh_qnO22rO"
      },
      "source": [
        "## Load VDVAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SBQuGxuS7NIO"
      },
      "outputs": [],
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-log.jsonl /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-log.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_J9IVYzW7dH1"
      },
      "outputs": [],
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-model-ema.th /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-model-ema.th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s8Gt12Sc7hGp"
      },
      "outputs": [],
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-model.th /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-model.th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JDNzMu5L7kQZ"
      },
      "outputs": [],
      "source": [
        "cp /content/drive/MyDrive/brain_decoding/data/vdvae/imagenet64-iter-1600000-opt.th /content/brain-decoding/vdvae/model/imagenet64-iter-1600000-opt.thz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phz_U8BY8A36",
        "outputId": "cb2516ab-200a-4266-9bad-a96f718ce779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ],
      "source": [
        "cd /content/brain-decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "IKRNN1_b22dg",
        "outputId": "40741f1e-29b3-4d68-8624-947b9b9bd1bb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-421339e2dc48>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdotdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_up_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Models Loading...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/brain-decoding/scripts/latest/../../vdvae/model_utils.py\u001b[0m in \u001b[0;36mset_up_data\u001b[0;34m(H)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#    eval_dataset = vaX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mshift_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshift_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "H = {'image_size': 64, 'image_channels': 3,'seed': 0, 'port': 29500, 'save_dir': './saved_models/test', 'data_root': './', 'desc': 'test', 'hparam_sets': 'imagenet64', 'restore_path': 'imagenet64-iter-1600000-model.th', 'restore_ema_path': 'vdvae/model/imagenet64-iter-1600000-model-ema.th', 'restore_log_path': 'imagenet64-iter-1600000-log.jsonl', 'restore_optimizer_path': 'imagenet64-iter-1600000-opt.th', 'dataset': 'imagenet64', 'ema_rate': 0.999, 'enc_blocks': '64x11,64d2,32x20,32d2,16x9,16d2,8x8,8d2,4x7,4d4,1x5', 'dec_blocks': '1x2,4m1,4x3,8m4,8x7,16m8,16x15,32m16,32x31,64m32,64x12', 'zdim': 16, 'width': 512, 'custom_width_str': '', 'bottleneck_multiple': 0.25, 'no_bias_above': 64, 'scale_encblock': False, 'test_eval': True, 'warmup_iters': 100, 'num_mixtures': 10, 'grad_clip': 220.0, 'skip_threshold': 380.0, 'lr': 0.00015, 'lr_prior': 0.00015, 'wd': 0.01, 'wd_prior': 0.0, 'num_epochs': 10000, 'n_batch': 4, 'adam_beta1': 0.9, 'adam_beta2': 0.9, 'temperature': 1.0, 'iters_per_ckpt': 25000, 'iters_per_print': 1000, 'iters_per_save': 10000, 'iters_per_images': 10000, 'epochs_per_eval': 1, 'epochs_per_probe': None, 'epochs_per_eval_save': 1, 'num_images_visualize': 8, 'num_variables_visualize': 6, 'num_temperatures_visualize': 3, 'mpi_size': 1, 'local_rank': 0, 'rank': 0, 'logdir': './saved_models/test/log'}\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "H = dotdict(H)\n",
        "\n",
        "H, preprocess_fn = set_up_data(H)\n",
        "\n",
        "print('Models Loading...')\n",
        "ema_vae = load_vaes(H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrsevukPttFa"
      },
      "source": [
        "## Extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aVDpqk0sddW"
      },
      "outputs": [],
      "source": [
        "midas_depth = MidasDetector.from_pretrained(\n",
        "  \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\"\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F52PIl_5t6rA"
      },
      "outputs": [],
      "source": [
        "keys = [\"test\", \"train\"]\n",
        "\n",
        "dataloaders = {\n",
        "    keys[0]: test_dl,\n",
        "    keys[1]: train_dl,\n",
        "}\n",
        "\n",
        "# stimulus_images = {\n",
        "#     keys[0]: [],\n",
        "#     keys[1]: []\n",
        "# }\n",
        "\n",
        "\n",
        "depth_images = {\n",
        "    keys[0]: [],\n",
        "    keys[1]: []\n",
        "}\n",
        "\n",
        "latents = {\n",
        "    keys[0]: [],\n",
        "    keys[1]: []\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egvhx4fWs2wP",
        "outputId": "c0ab5ae4-ad09-47e2-c676-ccfcc913cc4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features of test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "982it [05:07,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features of train data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8559it [44:53,  3.18it/s]\n"
          ]
        }
      ],
      "source": [
        "for data in keys:\n",
        "    print(f\"Extracting features of {data} data...\")\n",
        "    # passing data one at a time\n",
        "    for ndx, (_, image, _) in enumerate(tqdm(dataloaders[data])):\n",
        "\n",
        "        image_512 = F.interpolate(image, (512, 512), mode='bilinear', align_corners=False, antialias=True)\n",
        "        # stimulus_images[data].append(image_512) #(b,c,h,w)\n",
        "\n",
        "        image_depth = midas_depth(transforms.ToPILImage()(image[0]), detect_resolution=425, image_resolution=64)\n",
        "\n",
        "        print(type(image_depth), image_depth.shape)\n",
        "\n",
        "        break\n",
        "\n",
        "        image_depth = transforms.ToTensor()(image_depth).unsqueeze(0) #(b,c,h,w)\n",
        "        depth_images[data].append(image_depth)\n",
        "\n",
        "        data_input, target = preprocess_fn(image_depth.permute(0,2,3,1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            activations = ema_vae.encoder.forward(data_input)\n",
        "            px_z, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "            #recons = ema_vae.decoder.out_net.sample(px_z)\n",
        "            batch_latent = []\n",
        "            for i in range(num_latents):\n",
        "                #test_latents[i].append(stats[i]['z'].cpu().numpy())\n",
        "                batch_latent.append(stats[i]['z'].cpu().numpy().reshape(len(data_input),-1))\n",
        "\n",
        "            latents[data].append(np.hstack(batch_latent))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "depth_images_backup = {\n",
        "    keys[0]: depth_images[keys[0]],\n",
        "    keys[1]: depth_images[keys[1]],\n",
        "}\n",
        "\n",
        "latents_backup = {\n",
        "    keys[0]: latents[keys[0]],\n",
        "    keys[1]: latents[keys[1]],\n",
        "}\n",
        "\n",
        "print(len(depth_images[keys[0]]), len(depth_images[keys[1]]))\n",
        "print(len(latents[keys[0]]), len(latents[keys[1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgiYVtNCg-a-",
        "outputId": "d8c4ae14-1e58-42e4-9fbd-7f6ad9cf21e7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "982 8559\n",
            "982 8559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "depth_images[keys[0]] = torch.stack(depth_images[keys[0]])\n",
        "depth_images[keys[1]] = torch.stack(depth_images[keys[1]])\n",
        "\n",
        "latents[keys[0]] = np.concatenate(latents[keys[0]])\n",
        "latents[keys[1]] = np.concatenate(latents[keys[1]])"
      ],
      "metadata": {
        "id": "nzbPZ7JmOABC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(depth_images[keys[0]], f\"/content/drive/MyDrive/brain_decoding/data/fMRI/NSD/processed-data/subj{subj_id}/webdataset/depth_stim_test_sub{sub}.pt\")\n",
        "torch.save(depth_images[keys[1]], f\"/content/drive/MyDrive/brain_decoding/data/fMRI/NSD/processed-data/subj{subj_id}/webdataset/depth_stim_train_sub{sub}.pt\")\n"
      ],
      "metadata": {
        "id": "twv11oZ4Vlpy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez(\"/content/drive/MyDrive/brain_decoding/data/vdvae/subj{:02d}/nsd_vdvae_features_{}l_sub{}.npz\".format(sub,num_latents,sub),train_latents=latents[keys[1]],test_latents=latents[keys[0]])"
      ],
      "metadata": {
        "id": "pt7gzn8crOAt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(depth_images[keys[0]].shape, depth_images[keys[1]].shape)  #squeeze(1) => (b,c,h,w)\n",
        "print(latents[keys[0]].shape, latents[keys[1]].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "QzcZByqYVq1Q",
        "outputId": "8ca0d49b-682e-437d-da25-715c702e2bc7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3b181a6009cb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#squeeze(1) => (b,c,h,w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-kwqA8qzSEf"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8Cg8BeTxVqiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89991d03-4c09-4c07-b2e1-e1ac45956fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain-decoding\n"
          ]
        }
      ],
      "source": [
        "cd /content/brain-decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_nkLgaTqzYim"
      },
      "outputs": [],
      "source": [
        "import sklearn.linear_model as skl\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run if executing the cell below results into the error: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "\n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "YeMU9NxrJF0v"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vdvae latents\n",
        "!cp /content/drive/MyDrive/brain_decoding/data/vdvae/subj{subj_id}/nsd_vdvae_features_{num_latents}l_sub{sub}.npz /content/brain-decoding/data/extracted_features/subj{subj_id}\n",
        "# # fmri\n",
        "!cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/processed-data/subj{subj_id}/webdataset/nsd_fmriavg_nsdgeneral_sub{sub}.npz /content/brain-decoding/data/processed_data/subj{subj_id}"
      ],
      "metadata": {
        "id": "b5ZX0JzK9_nk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sgBgLxyYVyxQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373d524a-f05a-4a82-d9c0-db79afe1be6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8559, 353312) (982, 353312)\n"
          ]
        }
      ],
      "source": [
        "nsd_features = np.load('data/extracted_features/subj{:02d}/nsd_vdvae_features_{}l_sub{}.npz'.format(sub,num_latents,sub))\n",
        "train_latents = nsd_features['train_latents']\n",
        "test_latents = nsd_features['test_latents']\n",
        "print(train_latents.shape, test_latents .shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XS3BmN8EWPXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b383e4db-d311-4c4d-bea2-ad1269787604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8559, 14278) (982, 14278)\n"
          ]
        }
      ],
      "source": [
        "fmri = np.load('data/processed_data/subj{:02d}/nsd_fmriavg_nsdgeneral_sub{}.npz'.format(sub,sub))\n",
        "train_fmri = fmri['train_fmri']\n",
        "test_fmri = fmri['test_fmri']\n",
        "print(train_fmri.shape, test_fmri.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ML9AteZ1WR4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c91e8d-c805-4e32-9b8c-f0ebe4f90805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-3.1625003e-09 0.9999421\n",
            "-0.02679766 0.995617\n",
            "11.157454 -13.014394\n",
            "11.769697 -13.347021\n"
          ]
        }
      ],
      "source": [
        "## Preprocessing fMRI\n",
        "\n",
        "train_fmri = train_fmri/300\n",
        "test_fmri = test_fmri/300\n",
        "\n",
        "norm_mean_train = np.mean(train_fmri, axis=0)\n",
        "norm_scale_train = np.std(train_fmri, axis=0, ddof=1)\n",
        "train_fmri = (train_fmri - norm_mean_train) / norm_scale_train\n",
        "test_fmri = (test_fmri - norm_mean_train) / norm_scale_train\n",
        "\n",
        "print(np.mean(train_fmri),np.std(train_fmri))\n",
        "print(np.mean(test_fmri),np.std(test_fmri))\n",
        "\n",
        "print(np.max(train_fmri),np.min(train_fmri))\n",
        "print(np.max(test_fmri),np.min(test_fmri))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XbvBb3VwWUDI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RepeatedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if sub==1:\n",
        "#     alpha=40\n",
        "# elif sub==2:\n",
        "#     alpha=50\n",
        "# elif sub==5:\n",
        "#     alpha=60\n",
        "# elif sub==7:\n",
        "#     alpha=60\n",
        "\n",
        "alpha = 100"
      ],
      "metadata": {
        "id": "PpeCwwmeoWzb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VAK6NH4fWV48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3359483-a16a-4add-9663-2e4655f07098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training latents Feature Regression - sub 2 ...\n",
            "-0.01255250233297142\n"
          ]
        }
      ],
      "source": [
        "num_voxels, num_train, num_test = train_fmri.shape[1], len(train_fmri), len(test_fmri)\n",
        "\n",
        "## latents Features Regression\n",
        "print(f'Training latents Feature Regression - sub {sub} ...')\n",
        "\n",
        "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# reg = skl.RidgeCV(alphas=np.arange(10000, 100000, 10000), cv=cv, scoring='neg_mean_absolute_error', fit_intercept=True)\n",
        "\n",
        "reg = skl.Ridge(alpha=alpha*1000, max_iter=10000, fit_intercept=True)\n",
        "reg.fit(train_fmri, train_latents)\n",
        "pred_test_latent = reg.predict(test_fmri)\n",
        "std_norm_test_latent = (pred_test_latent - np.mean(pred_test_latent,axis=0)) / np.std(pred_test_latent,axis=0)\n",
        "pred_latents = std_norm_test_latent * np.std(train_latents,axis=0) + np.mean(train_latents,axis=0)\n",
        "print(reg.score(test_fmri,test_latents))\n",
        "\n",
        "# print(reg.score(test_fmri,test_latents), 'alpha: %f' % reg.alpha_)\n",
        "\n",
        "np.save('/content/drive/MyDrive/brain_decoding/data/vdvae/subj{:02d}/nsd_vdvae_nsdgeneral_pred_sub{}_alpha{}k.npy'.format(sub,sub,alpha),pred_latents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GrYatNQlWX58"
      },
      "outputs": [],
      "source": [
        "datadict = {\n",
        "    'weight' : reg.coef_,\n",
        "    'bias' : reg.intercept_,\n",
        "\n",
        "}\n",
        "\n",
        "with open('data/regression_weights/subj{:02d}/lvae_regression_weights_alpha{}k.pkl'.format(sub,alpha),\"wb\") as f:\n",
        "  pickle.dump(datadict,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsStngb-e94I"
      },
      "source": [
        "## Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZAlDoq9HWZbv"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/brain_decoding/data/vdvae/subj{subj_id}/nsd_vdvae_nsdgeneral_pred_sub{sub}_alpha{alpha}k.npy /content/brain-decoding/data/predicted_features/subj{subj_id}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/brain_decoding/data/fMRI/NSD/processed-data/subj{subj_id}/webdataset/depth_stim_test_sub{sub}.pt /content/brain-decoding/data/processed_data/subj{subj_id}"
      ],
      "metadata": {
        "id": "MyK5d0BWQvwU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = torch.load('data/processed_data/subj{:02d}/depth_stim_test_sub{}.pt'.format(sub,sub))\n",
        "test_images = test_images.squeeze(1).permute(0,2,3,1)\n",
        "print(type(test_images), test_images.shape, test_images.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRPXCiijQq0P",
        "outputId": "f69aa4ec-ce29-4538-ab76-e3b6e6f1001b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> torch.Size([982, 64, 64, 3]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNnznRdacpKf",
        "outputId": "56a3b306-5b7e-4144-856a-574f51ee650c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([982, 64, 64, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.ToPILImage()(test_images[0].permute(2,0,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "COokrTvOchGd",
        "outputId": "c59321f5-b2b9-42fb-c091-19d75a56b315"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_images = batch_generator_external_images(test_images)\n",
        "batch_size = 30\n",
        "testloader = DataLoader(test_images,batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "QuOZH3xwRClo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_latents = []\n",
        "for i,x in enumerate(tqdm(testloader)):\n",
        "  data_input, target = preprocess_fn(x)\n",
        "  with torch.no_grad():\n",
        "        # print(i*batch_size)\n",
        "        activations = ema_vae.encoder.forward(data_input)\n",
        "        px_z, stats = ema_vae.decoder.forward(activations, get_latents=True)\n",
        "        #recons = ema_vae.decoder.out_net.sample(px_z)\n",
        "        batch_latent = []\n",
        "        for i in range(num_latents):\n",
        "            batch_latent.append(stats[i]['z'].cpu().numpy().reshape(len(data_input),-1))\n",
        "        test_latents.append(np.hstack(batch_latent))\n",
        "        #stats_all.append(stats)\n",
        "        #imshow(imgrid(recons, cols=batch_size,pad=20))\n",
        "        #imshow(imgrid(test_images[i*batch_size : (i+1)*batch_size], cols=batch_size,pad=20))\n",
        "test_latents = np.concatenate(test_latents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuLqcDX3S2Zc",
        "outputId": "225d578a-ad6d-4a2e-8c1b-60122b435517"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33/33 [00:20<00:00,  1.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_latents = np.load('data/predicted_features/subj{:02d}/nsd_vdvae_nsdgeneral_pred_sub{}_alpha{}k.npy'.format(sub,sub,alpha))\n",
        "ref_latent = stats"
      ],
      "metadata": {
        "id": "eGzp2ow-kgcz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform latents from flattened representation to hierarchical\n",
        "def latent_transformation(latents, ref):\n",
        "  layer_dims = np.array([2**4,2**4,2**8,2**8,2**8,2**8,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**10,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**12,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14,2**14])\n",
        "  transformed_latents = []\n",
        "  for i in range(num_latents):\n",
        "    t_lat = latents[:,layer_dims[:i].sum():layer_dims[:i+1].sum()]\n",
        "    #std_norm_test_latent = (t_lat - np.mean(t_lat,axis=0)) / np.std(t_lat,axis=0)\n",
        "    #renorm_test_latent = std_norm_test_latent * np.std(kamitani_latents[i][num_test:].reshape(num_train,-1),axis=0) + np.mean(kamitani_latents[i][num_test:].reshape(num_train,-1),axis=0)\n",
        "    c,h,w=ref[i]['z'].shape[1:]\n",
        "    transformed_latents.append(t_lat.reshape(len(latents),c,h,w))\n",
        "  return transformed_latents\n",
        "\n",
        "def sample_from_hier_latents(latents,sample_ids):\n",
        "  sample_ids = [id for id in sample_ids if id<len(latents[0])]\n",
        "  layers_num=len(latents)\n",
        "  sample_latents = []\n",
        "  for i in range(layers_num):\n",
        "    sample_latents.append(torch.tensor(latents[i][sample_ids]).float().cuda())\n",
        "  return sample_latents"
      ],
      "metadata": {
        "id": "ot3YD5S-kp8L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = len(test_images)\n",
        "num_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-0sHu8mamXD",
        "outputId": "ffe851f5-6210-4e1b-f628-05936a0491bb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "982"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = range(num_test)\n",
        "input_latent = latent_transformation(pred_latents[idx],ref_latent)"
      ],
      "metadata": {
        "id": "xMlLrJOnkthT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ndx in tqdm(range(int(np.ceil(num_test/batch_size)))):\n",
        "  # print(ndx*batch_size)\n",
        "  samp = sample_from_hier_latents(input_latent,range(ndx*batch_size,(ndx+1)*batch_size))\n",
        "  px_z = ema_vae.decoder.forward_manual_latents(len(samp[0]), samp, t=None)\n",
        "  sample_from_latent = ema_vae.decoder.out_net.sample(px_z)\n",
        "  upsampled_images = []\n",
        "  for idx in range(len(sample_from_latent)):\n",
        "      im = sample_from_latent[idx]\n",
        "      im = Image.fromarray(im)\n",
        "      im = im.resize((512,512),resample=3)\n",
        "      im.save('/content/drive/MyDrive/brain_decoding/data/reconstructed/vdvae/subj{:02d}/{}.png'.format(sub,ndx*batch_size+idx))"
      ],
      "metadata": {
        "id": "PnnEksL-lGln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13da2b0-c29b-47f7-b746-b534a7625d87"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33/33 [00:22<00:00,  1.47it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}